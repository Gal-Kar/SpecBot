import json, sys, base64, os
from lib.helper.helper import *
from lib.xss import *
from lib.sqli import *
from lib import requester
from random import randint
from multiprocessing import Pool
from huepy import *
from urllib.parse import unquote
from tqdm import tqdm
import signal

sys.path.append(path_based_on_os(os.path.abspath(os.path.dirname(os.path.abspath(__file__)) + "/..")))
from aws_helper import upload_dict_to_s3, send_to_sqs, poll_from_sqs, download_from_s3
from constants import VULNERABILITY_SCANNER_SQS, PARENT_BUCKET, VULNERABILITY_SCANNER_FILENAME, VULNERABILITY_SCANNER_SQS_RESULT, VULNERABILITY_SCANNER_BUCKET

is_cloud = 'CLOUD' in os.environ
signal.signal(signal.SIGINT, signal_handler)

def generate_file_to_export(url, path_to_export_file):
	parsed_link = urlparse(url)
	base_url = parsed_link.scheme + "://" + parsed_link.netloc
	encoded_bytes = base64.b64encode(base_url.encode("utf-8"))
	encoded_url = encoded_bytes.decode("utf-8")

	return validate_path(path_to_export_file) + encoded_url + '.json'

def xss_runner(url, proxy, cookie, url_data, path_to_payloads):
	return xss.main(url, proxy, cookie, url_data, path_to_payloads)

def sqli_runner(url, path_to_payloads):
	return sqli.main(url, path_to_payloads)

def path_vulnerability(url, url_data):
	if (url_data.get('suspect') == 'true'):
		return [{ 'url': url.split('?')[0], 'message': 'message', 'type': 'Path Vulnerability', 'priority': 'Medium' }]
	return None

def start_cloud():
	crawler_bucket_path = poll_from_sqs(VULNERABILITY_SCANNER_SQS)
	crawler_data = download_from_s3(PARENT_BUCKET, crawler_bucket_path)

	parsed_url = urlparse(crawler_bucket_path)
	base_url = parsed_url.scheme + "://" + parsed_url.netloc
	result_file_path = f'{base_url}/{VULNERABILITY_SCANNER_FILENAME}'

	cwd = os.getcwd()
	xss_payload_path = f'{cwd}/VulnerabilityScanner/data/payloads/xss.txt'
	sqli_payload_path = f'{cwd}/VulnerabilityScanner/data/payloads/sqli.txt'

	all_urls = crawler_data['all_urls']
	results = []
	for url in all_urls:
		if (url.startswith('https://') or url.startswith('http://')):
			results.append(xss_runner(url, proxy, cookie, crawler_data[url], xss_payload_path))
			results.append(sqli_runner(url, sqli_payload_path))
			results.append(path_vulnerability(url, crawler_data[url]))

	security_parser_result = {}
	for result in results:
		if result is not None:
			for item in result:
				if item is not None:
					add_to_json(security_parser_result, item['url'], item['priority'], item['type'], -1, -1, -1, item['message'], [], item['suggestions'])

	upload_dict_to_s3(VULNERABILITY_SCANNER_BUCKET, result_file_path, security_parser_result)
	print(f'Uploading bucket path to {VULNERABILITY_SCANNER_SQS_RESULT} SQS')

	send_to_sqs(VULNERABILITY_SCANNER_SQS_RESULT, result_file_path)
	print('Done')

def start_local():
	argv = sys.argv
	filename = argv[1]
	path_to_export_file = argv[2]

	with open(filename, 'r') as fp:
		data = json.load(fp)
	all_urls = data['all_urls']

	file_to_export = generate_file_to_export(all_urls[0], path_to_export_file)
	results = []

	with Pool() as p:
		for url in all_urls:
			if (url.startswith('https://') or url.startswith('http://')):
				results.append(p.apply_async(xss_runner, args=(url, proxy, cookie, data[url], 'data/payloads/xss.txt')))
				results.append(p.apply_async(sqli_runner, args=(url, 'data/payloads/sqli.txt')))
				results.append(p.apply_async(path_vulnerability, args=(url, data[url])))

		p.close()
		p.join()

	for result in results:
		if result.get() is not None:
			for item in result.get():
				if item is not None:
					write_to_json(item['url'], item['priority'], item['type'], -1, -1, -1, item['message'], [], item['suggestions'], file_to_export)

	print('done')

if __name__== "__main__":
	if is_cloud:
		while True:
			start_cloud()
	else:
		start_local()